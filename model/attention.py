import torch
from torch.nn import Module

class MultiHeadAttention(Module):

    def __init__(self, d_model, n
                 , d_k, d_v, n_heads
                 , dropout=0.1):    
        super(MultiHeadAttention, self).__init__()
        pass
    

    def forward(self, x):
        pass

